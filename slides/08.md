---
title: "Теория информационного поиска"
subtitle: "Лекция 8. Оценка качества поиска"
date: 05.05.2025
author: "Дмитрий Грановский"
institute: "СПбГУ"
theme: AnnArbor
colortheme: dolphin
aspectratio: 169
mainfont: "Ubuntu"
mathfont: "Ubuntu"
monofont: "Ubuntu Mono"
fontsize: 14pt
colorlinks: true
header-includes:
	\usepackage{xcolor,colortbl}
	\renewcommand{\arraystretch}{1.5}
---

## Релевантность

- задает ранжирование
+ безразмерная величина, важно только сравнивать документы между собой
+ считается для пары (запрос, документ)
	+  точнее: для тройки (запрос, документ, **пользовательский контекст**)
+ бывает разная:
	+ бинарная и произвольная
	+ пользовательская и вычисленная

## Общий подход

- оценка = сравнение с идеальной системой
+ идеальная система:
	+ показывает только релевантные документы
	+ самые релевантные ранжирует выше
	+ может иметь дополнительные требования: свежесть, разнообразие и т.д.
	+ не существует :-(
+ оценка — численная, состоит в расчете **метрики**

## Общий подход конкретнее

- оцениваем качество по корзине запросов и усредняем
	+ составить хорошую корзину — отдельная задача
+ как оценить качество по одному запросу:
	+ попросим экспертов (*асессоров*) оценить какое-то множество документов на релевантность запросу
		+ желательно много асессоров и $>1$ асессора на&nbsp;документ
	+ эти оценки используем для расчета метрик (подробности далее)

## Оценка качества булева (двоичного) поиска

- далее везде считаем, что смотрим на один запрос `q`
+ $Rel(D_i, q) \in \{0, 1\}$
+ рассматриваем ситуацию:
	+ асессоры: релевантны документы $d_{i_1}\dots d_{i_n}$
	+ система: нашла документы $d_{j_1} \dots d_{j_k}$
	+ ранжирования нет
+ возможные состояния документа:
	+ релевантный, нашелся
	+ нерелеватный, нашелся
	+ релевантный, не нашелся
	+ нерелевантный, не нашелся (их большинство)

## Оценка

```{=latex}
\begin{tabular}{|*{3}{l|}}
\hline
& \textbf{Rel} & \textbf{NRel} \\
\hline
\textbf{нашли} & true positive (tp) & false positive (fp) \\
\hline
\textbf{не нашли} & false negative (fn) & true negative (tn) \\
\hline
$\Sigma$ & $R$ & $N - R$ \\
\hline
\end{tabular}
```

. . .

+ мы не можем оценить всю коллекцию
+ поэтому обычно оцениваем верх фактической выдачи
+ количество **tn** нам безразлично

## Типы ошибок

```{=latex}
\begin{tabular}{|*{3}{l|}}
\hline
& \textbf{Rel} & \textbf{NRel} \\
\hline
\textbf{нашли} & true positive (tp) & false positive (fp) \\
\hline
\textbf{не нашли} & false negative (fn) & true negative (tn) \\
\hline
\end{tabular}
```

. . .

- булев поиск можно рассмотреть как задачу *классификации* на&nbsp;2&nbsp;класса (по релевантности)
+ false positive = ошибка I рода
+ false negative = ошибка II рода
+ таблица называется матрицей несоответствий/ошибок (*confusion matrix*)

## Метрики классификации

```{=latex}
\begin{tabular}{|*{3}{l|}}
\hline
& \textbf{Rel} & \textbf{NRel} \\
\hline
нашли & true positive (\textcolor{green}{tp}) & false positive (\textcolor{red}{fp}) \\
\hline
не нашли & false negative (\textcolor{orange}{fn}) & true negative (tn) \\
\hline
\end{tabular}
```

. . .

Две фундаментальные метрики:

+ точность (*precision*): $P = {\textcolor{green}{tp} \over \textcolor{green}{tp} + \textcolor{red}{fp}}$
+ полнота (*recall*): $R = {\textcolor{green}{tp} \over \textcolor{green}{tp} + \textcolor{orange}{fn}}$
+ ещё бывает *accuracy*: $Acc = {\textcolor{green}{tp} + tn \over \textcolor{green}{tp} + tn + \textcolor{red}{fp} + \textcolor{orange}{fn}}$

## Точность и полнота

- идеальная классификация недостижима
+ почти всегда можем сдвигать качество либо к точности с потерей полноты, либо наоборот (tradeoff)
+ в некоторых задачах важнее точность, в некоторых полнота
+ если мы понимаем, в какой пропорции важны точность и полнота, можно использовать *$F$-меру* (*f-score*)
+ в частности, $F_1$-меру: $F_1(P, R) = {2 \times P \times R \over P + R}$
+ пример: $F_1(0.3, 0.1) = {2 \times 0.3 \times 0.1 \over 0.3 + 0.1} = 0.15$

## Оценка качества поиска с ранжированием

+ $Rel(d_i, q) \in [0, 1]$
+ рассматриваем ситуацию:
	+ асессоры: релевантность документа $d_i = Rel_{d_i}$
	+ система: нашла документы $d_{j_1} \dots d_{j_k}$ в этом порядке
+ набор дискретных оценок релевантности, например:
	+ Vital (в статье $Rel = 0.4$)
	+ Useful
	+ Rel+
	+ Rel-
	+ Stupid


## Precision @k

```{=latex}
\begin{tabular}{|*{4}{l|}}
\hline
\textbf{k} & \textbf{Rel?} & \textbf{Всего Rel} & \textbf{P@k} \\
\hline
1 & \textcolor{green}{R} & 1 & ${1 \over 1} = 1.0$ \\ 
\hline
2 & \textcolor{red}{NR} & 1 & ${1 \over 2} = 0.5$ \\
\hline
3 & \textcolor{red}{NR} & 1 & ${1 \over 3} \approx 0.33$ \\
\hline
4 & \textcolor{green}{R} & 2 & ${2 \over 4} = 0.5$ \\ 
\hline
\end{tabular}
```

## Precision @k, пример #2

```{=latex}
\begin{tabular}{|*{4}{l|}}
\hline
\textbf{k} & \textbf{Rel?} & \textbf{Всего Rel} & \textbf{P@k} \\
\hline
1 & \textcolor{green}{R} & 1 & ${1 \over 1} = 1.0$ \\ 
\hline
2 & \textcolor{green}{R} & 2 & ${2 \over 2} = 1.0$ \\ 
\hline
3 & \textcolor{red}{NR} & 2 & ${2 \over 3} \approx 0.67$ \\
\hline
4 & \textcolor{red}{NR} & 2 & ${2 \over 4} = 0.5$ \\
\hline
\end{tabular}
```

. . .

Проблема: эта выдача лучше предыдущей, а `P@4` такой же

## Average precision

```{=latex}
\begin{tabular}{|*{3}{l|}}
\hline
\textbf{k} & \textbf{Rel?} & \textbf{AP@k} \\
\hline
1 & \textcolor{green}{R} & ${{1 \times 1/1}\over 1} = 1.0$ \\ 
\hline
2 & \textcolor{red}{NR} & ${{(1 \times 1/1) + (0 \times 1/2)} \over 2} = 0.5$ \\
\hline
3 & \textcolor{red}{NR} & ${{(1 \times 1/1) + (0 \times 1/2) + (0 \times 1/3)} \over 3} \approx 0.33$ \\
\hline
4 & \textcolor{green}{R} & ${{(1 \times 1/1) + (0 \times 1/2) + (0 \times 1/3) + (1 \times 2/4)} \over 4} = 0.375$ \\ 
\hline
\end{tabular}
```

## Average precision @k, пример #2

```{=latex}
\begin{tabular}{|*{3}{l|}}
\hline
\textbf{k} & \textbf{Rel?} & \textbf{AP@k} \\
\hline
1 & \textcolor{green}{R} & ${{1 \times 1/1}\over 1} = 1.0$ \\ 
\hline
2 & \textcolor{green}{R} & ${{(1 \times 1/1) + (1 \times 2/2)} \over 2} = 1.0$ \\
\hline
3 & \textcolor{red}{NR} & ${{(1 \times 1/1) + (1 \times 2/2) + (0 \times 2/3)} \over 3} \approx 0.67$ \\
\hline
4 & \textcolor{red}{NR} & ${{(1 \times 1/1) + (1 \times 2/2) + (0 \times 2/3) + (0 \times 2/4)} \over 4} = 0.5$ \\ 
\hline
\end{tabular}
```

. . .

`mAP@k` (mean `AP@k`) — усредненное по корзине запросов

## Оценка ранжированной выдачи: DCG

- `CG@K` (Cumulative Gain at $k$) \
 $CG_k = \sum\limits_{i=1}^{k} rel_i$
+ `DCG@K` (Discounted Cumulative Gain at $k$) \
 $DCG_k = \sum\limits_{i=1}^{k} {{rel_i}\over{\log_2(i+1)}}$ или 
+ $DCG_k = \sum\limits_{i=1}^{k} {{2^{rel_i} - 1}\over{\log_2(i+1)}}$

## Оценка ранжированной выдачи: DCG

`NDCG@k` (Normalized `DCG@k`)

$NDCG_k = {{DCG_k}\over{IDCG_k}}$

, где $IDCG$ — Ideal DCG:

$IDCG_k = \sum\limits_{i=1}^{|REL_k|}{2^{rel_i} - 1\over{\log_2(i+1)}}$

, где $REL_k$ — все релевантные документы, отсортированные по убыванию релевантности

## Оценка ранжированной выдачи: pFound

$pFound_k = \sum\limits_{i=1}^{k} pLook_i \times pRel_i$, где:

+ $pLook_i$ — вероятность просмотреть $i$-й документ
+ $pRel_i$ — вероятность, что $i$-й документ релевантный

## Оценка ранжированной выдачи: pFound

$pLook_i = pLook_{i-1} \times (1 - pRel_{i-1}) \times (1 - pBreak)$

Идея этой формулы:

+ пользователь просматривает результаты сверху вниз
+ он прекращает просмотр, найдя релевантный документ
+ ни у какого документа нет $pRel_i = 1$
	+ в статье единственное ненулевое значение $0.4$
+ с вероятностью $pBreak$ он прекращает просмотр просто так («надоело» &copy;)
	+ в статье $pBreak = 0.15$

## Другие метрики, которые не влезли

+ ROC-AUC (площадь под кривой ошибок)
+ MRR (mean reciprocal rate)

## Онлайн

- все метрики выше считаются **оффлайн**, по оценкам релевантности
- но иногда лучше/проще смотреть на поведение пользователей онлайн
	- соответствующие *онлайн-метрики*, например:
		- CTR,
		- время до клика,
		- средняя позиция первого клика,
		- $\dots$
	- часто в виде *A/B тестирования*