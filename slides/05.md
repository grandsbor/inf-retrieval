---
title: "Теория информационного поиска"
subtitle: "Лекция 5. Векторная модель ранжирования"
date: 21.04.2025
author: "Дмитрий Грановский"
institute: "СПбГУ"
theme: AnnArbor
colortheme: dolphin
aspectratio: 169
mainfont: "Ubuntu"
mathfont: "Ubuntu"
monofont: "Ubuntu Mono"
fontsize: 14pt
colorlinks: true
---

## Проблемы булева поиска

1. Язык запросов — не для обычных людей
2. Равнозначность всех термов
3. Находим слишком мало (`AND`) или слишком много (`OR`)
4. Ранжирование требует
	- добавления сущностей:
		- зон, атрибутов, статического ранга
	- и/или учета расстояний

## Взвешивание терминов: tf

Идея 1:

- чем чаще слово запроса встречается в документе, тем, возможно, лучше (релевантнее) этот документ
- обозначим за $tf_{t, d}$ (*term frequency*)
- но не линейно релевантнее, а слабее
	+ например, можно взять логарифм: \
	$w_{t, d} = \log{tf_{t, d}} + 1$,  если $tf_{t,d} > 0$ \
	и $w_{t,d} = 0$ иначе
- уже можно использовать для подсчета релевантности: $Rel_{q,d} = \sum\limits_{t \in {q \cap d}} w_{t, d}$ (сумма по всем терминам)

## Взвешивание терминов: idf

Идея 2:

- редкие термины обычно информативнее частотных (см.&nbsp;*стоп-слова*)
- давайте давать редким больше веса
- согласимся, что важна не суммарная частота, а количество документов с термом: назовем его $df_t$ (*document frequency*)
- скорее всего, эта зависимость тоже нелинейная
- чем больше $df_t$, тем меньший коэффициент хотим давать
- $idf_t = \log{N \over df_t}$ (*inverse document frequency*)

## Взвешивание терминов: TF-IDF

Объединим идеи 1 и 2:

- $TF\text{-}IDF_{t, d} = tf_{t,d} \times idf_t$
- самая популярная модель взвешивания
- это один из многих вариантов (разное сглаживание и&nbsp;пр.)
- резюме: TF-IDF растет 1) с ростом числа вхождений слова в&nbsp;документ и 2) со степенью редкости термина
- можно обобщить на n-граммы (как?)

. . .

`tfidf = sklearn.feature_extraction.text.TfidfVectoriser(min_df=2, ngram_range=(1,3))`

## Переход к целому запросу

- пока умеем считать $Rel_{t, d}$, а хотим $Rel_{q, d}$ (некое число)
- чем может быть $Rel_{q, d}$?
	- степенью сходства/близости $q$ и $d$
	- вероятностью, что $d$ — релевантный документ
	- (остальное менее популярно)

## Вероятностные модели

- подробно рассматривать не будем
- самая известная модель — Okapi BM25: \
    $\sum\limits_{i \in Q} \log{{(r_i + {1\over2})/(R - r_i + {1 \over 2})} \over {(n_i - r_i + {1 \over 2})/(N - n_i - R + r_i + {1 \over 2})}} \cdot {{(k_1+1)f_i} \over {K+f_i}} \cdot {{(k_2+1)qf_i} \over {k_2+qf_i}}$
- концептуально похожа на TF-IDF
- есть улучшенная разновидность BM25F
- основана на модели бинарной независимости (BIM)

## Векторные модели

Общая идея:

- вспомним линейную алгебру: \
	$(t_1, t_2, \dots, t_k)$ — вектор в $k$-мерном пространстве
+ придумаем, как представить запросы и документы в одном и том же пространстве
+ после этого сможем получать численную меру сходства
+ самая популярная такая мера — косинус угла между векторами: \
	$score(q, d) = {{(Q, D)} \over {\|Q\| \cdot \|D\|}}$

## Векторная модель: пример

| термы/документы | $D_1$ | $D_2$ | $D_3$ |
| -- | -- | -- | -- |
| в | 5 | 2 | 10 |
| время | 5 | 2 | 0
| мост | 0 | 7 | 8
| петербург | 5 | 15 | 25
| разводка | 1 | 4 | 0

$q_1$ = `[разводка мостов петербург]`

## Векторная модель: пример

$q_1$ = `[разводка мостов петербург]`

\vfill

$score(\textcolor{red}{q_1}, D_1) = {{\textcolor{red}{1} \times 1 + \textcolor{red}{1} \times 0 + \textcolor{red}{1} \times 5} \over {\sqrt{\textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2}} \times \sqrt{1^2 + 0^2 + 5^2}}} \approx 0.679$

. . .

$score(\textcolor{red}{q_1}, D_2) = {{\textcolor{red}{1} \times 4 + \textcolor{red}{1} \times 7 + \textcolor{red}{1} \times 15} \over {\sqrt{\textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2}} \times \sqrt{4^2 + 7^2 + 15^2}}} \approx \textcolor{cyan}{0.881}$

$score(\textcolor{red}{q_1}, D_3) = {{\textcolor{red}{1} \times 0 + \textcolor{red}{1} \times 8 + \textcolor{red}{1} \times 25} \over {\sqrt{\textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2}} \times \sqrt{0^2 + 8^2 + 25^2}}} \approx 0.726$

## Векторная модель: пример

$q_2$ = `[время разводки мостов в петербурге]`

\vfill

. . .

$score(\textcolor{red}{q_2}, D_1) = {{\textcolor{red}{1} \times 5 + \textcolor{red}{1} \times 1 + \textcolor{red}{1} \times 0 + \textcolor{red}{1} \times 5 + \textcolor{red}{1} \times 5} \over {\sqrt{\textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2}} \times \sqrt{5^ 2 + 1^2 + 0^2 + 5^2 + 5^2}}} \approx 0.821$

. . .

$score(\textcolor{red}{q_2}, D_2) = \dots \approx 0.777$

$score(\textcolor{red}{q_2}, D_3) = \dots \approx 0.685$

## Векторная модель: взвешивание по TF-IDF

$q_2$ = `[время разводки мостов в петербурге]`

\vfill

<!--
По данным ruscorpora.ru на 20.04.2025:

df(в) = 129829
df(время) = 73585
df(мост) = 8116
df(петербург) = 14267
df(разводка) = 179

Примем N = 130000, исходя из df(в)

idf(в) = log(130000 / 129829) ~ 0.0006
idf(время) = log(130000 / 73585) ~ 0.247
idf(мост) = log(130000 / 8116) ~ 1.205
idf(петербург) = log(130000 / 14267) ~ 0.960
idf(разводка) = log(130000 / 179) ~ 2.861
-->

\small

$score_{tfidf}(\textcolor{red}{q_2}, D_1) =$

$= {{\textcolor{red}{1} \times {5 \times \textcolor{teal}{0.0006}} + \textcolor{red}{1} \times {1 \times \textcolor{teal}{0.25}} + \textcolor{red}{1} \times {0 \times \textcolor{teal}{1.2}} + \textcolor{red}{1} \times {5 \times \textcolor{teal}{0.96}} + \textcolor{red}{1} \times {5 \times \textcolor{teal}{2.86}}} \over {\sqrt{\textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2} + \textcolor{red}{1^ 2}} \times \sqrt{({5 \times \textcolor{teal}{0.0006}})^2 + ({1 \times \textcolor{teal}{0.25}})^2 + 0^2 + ({5 \times \textcolor{teal}{0.96}})^2 + ({5 \times \textcolor{teal}{2.86}})^2}}} \approx$

\begin{center}$\approx 0.574$\end{center}

. . .

$score_{tfidf}(\textcolor{red}{q_2}, D_2) = \dots \approx \textcolor{cyan}{0.768}$

$score_{tfidf}(\textcolor{red}{q_2}, D_3) = \dots \approx 0.581$


## Векторные модели: плюсы

- простые
- можно выбирать пространство, например, включать n-граммы или выбрасывать стоп-слова,
- можно использовать любую схему взвешивания (например, $TF\text{-}IDF$); по умолчанию все координаты имеют одинаковый вес
- можно использовать любую меру сходства векторов

## Векторные модели: минусы

- независимость терминов: мешок слов \
	(шаг назад по сравнению с координатным индексом)
- по-прежнему требуется точное совпадение лексем в запросе и документе (*чинить* != *ремонтировать*)
- всё еще не можем получить оптимальное ранжирование, особенно для однословных запросов
- сравнивать вектор запроса с вектором каждого документа коллекции — слишком долго
- первые две проблемы решает `word2vec`

## Фильтрация и ранжирование

- идея:
	- на первой стадии используем быстрый булев поиск, чтобы выбрать потенциально подходящие документы
		- возможно, с координатным индексом
	- на второй стадии ранжируем выбранные документы согласно векторной модели
- требуется добавление данных в индекс
- стадий может быть больше двух
- на самом деле можно применять для любых моделей

